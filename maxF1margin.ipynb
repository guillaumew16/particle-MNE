{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Logging\n",
    "logger = ConsoleLogger(stdout)\n",
    "# debuglogger = ConsoleLogger(stderr, Logging.Debug)\n",
    "global_logger(logger)\n",
    "using JLD\n",
    "\n",
    "using Random\n",
    "rng = MersenneTwister(1234)\n",
    "import Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "pyplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "using ProgressMeter\n",
    "using LinearAlgebra: dot\n",
    "include(\"utils/misc.jl\")\n",
    "using .MiscUtils: myCircle, mySphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = Dates.now()\n",
    "\n",
    "## model parameters\n",
    "input_dim = 2\n",
    "bias = true\n",
    "m = 50  # nb hidden neurons for each sign\n",
    "N = 5   # nb samples\n",
    "activation_str = \"ReLUcub\"\n",
    "\n",
    "## logging\n",
    "ts_fsfriendly = Dates.format(ts, \"yyyy-mm-ddTHHMMSS\") # filesystem-friendly string for ts\n",
    "resultdir = mkpath(\"results/maxF1margin__dim$(input_dim)bias$(bias)__m$(m)__N$(N)__$(activation_str)__$(ts_fsfriendly)\")\n",
    "\n",
    "logfile = \"$resultdir/log.txt\"\n",
    "touch(logfile)\n",
    "open(logfile, \"a\") do f\n",
    "    write(f, \"input_dim=$input_dim\\n\")\n",
    "    write(f, \"bias: $bias\\n\")\n",
    "    write(f, \"m=$m\\n\")\n",
    "    write(f, \"N=$N\\n\")\n",
    "    write(f, \"activation: $activation_str\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 8\n",
    "scaling = 1 # unknown what the \"correct\" scaling typically is!\n",
    "\n",
    "## algo parameters\n",
    "T = Int(floor(4000 / (alpha^scaling)))\n",
    "eta0_a = 1e-1 * alpha # initial stepsize\n",
    "eta0_w = 1e-1 * alpha\n",
    "eta0_theta = 1e-2 * alpha\n",
    "constrain_theta = true\n",
    "extrasteps = 2 # extrasteps=1: CP-MDA, extrasteps=2: CP-MP\n",
    "\n",
    "open(logfile, \"a\") do f\n",
    "    write(f, \"T=$T\\n\")\n",
    "    write(f, \"eta0_a=$(eta0_a)\\n\")\n",
    "    write(f, \"eta0_w=$(eta0_w)\\n\")\n",
    "    write(f, \"eta0_theta=$(eta0_theta)\\n\")\n",
    "    write(f, \"constrain_theta: $(constrain_theta)\\n\")\n",
    "    write(f, \"extrasteps=$(extrasteps)\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting parameters\n",
    "TNI_min = 1 # plot NI error\n",
    "TNI_max = T\n",
    "evalNIevery = Int(floor((TNI_max-TNI_min)/50))\n",
    "Ntheta = Int(1e4)\n",
    "skip_avg = true # avg not implemented\n",
    "\n",
    "@assert input_dim==2\n",
    "# r = range(-3, 3, length=101)\n",
    "r = range(-2.2, 2.2, length=101)\n",
    "\n",
    "Tplotreg_min = 1 # plot decision regions gif\n",
    "Tplotreg_max = T\n",
    "plotregevery = Int(floor((Tplotreg_max-Tplotreg_min)/50))\n",
    "hidetitle = false\n",
    "hidelabel = false\n",
    "skip_gifs = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if activation_str == \"ReLU\"\n",
    "    activation(x) = max(0, x)\n",
    "elseif activation_str == \"abs\"\n",
    "    activation(x) = abs(x)\n",
    "elseif activation_str == \"ReLUsq\"\n",
    "    activation(x) = max(0, x)^2\n",
    "elseif activation_str == \"ReLUcub\"\n",
    "    activation(x) = max(0, x)^3\n",
    "elseif activation_str == \"ReLUquar\"\n",
    "    activation(x) = max(0, x)^4\n",
    "# elseif activation_str == \"sq\" # polynomial activation is silly\n",
    "#     activation(x) = x^2\n",
    "elseif activation_str == \"sigmoid\"\n",
    "    activation(x) = 1/(1+exp(-x))\n",
    "elseif activation_str == \"tanh\"\n",
    "    activation(x) = tanh(x)\n",
    "else\n",
    "    error(\"activation_str not recognized, should be \n",
    "        \\\"ReLU\\\" or \n",
    "        \\\"abs\\\" or \n",
    "        \\\"ReLUsq\\\" or \n",
    "        \\\"ReLUcub\\\" or \n",
    "        \\\"ReLUquar\\\" or \n",
    "        \\\"sigmoid\\\" \n",
    "        or \\\"tanh\\\"\")\n",
    "end\n",
    "\n",
    "if bias == true\n",
    "    d = input_dim + 1\n",
    "else\n",
    "    d = input_dim\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function makedataset(d, N; bias, rng=MersenneTwister(1234))\n",
    "    x = randn(rng, d, N)\n",
    "    if bias\n",
    "        x[d,:] .= 1\n",
    "    end\n",
    "    y = sign.(rand(rng, N).-0.5) # -1 or 1 uniformly\n",
    "    return x, y\n",
    "end\n",
    "\n",
    "Random.seed!(rng, 1234)\n",
    "x, y = makedataset(d, N; bias=bias, rng=rng)\n",
    "save(\"$resultdir/dataset.jld\", \"x\", x, \"y\", y)\n",
    "\n",
    "scatter(x[1, y.==1], x[2, y.==1], m=:circ, markersize=8, label=\"+\", color=:green)\n",
    "scatter!(x[1, y.==-1], x[2, y.==-1], m=:utriangle, markersize=8, label=\"-\", color=:red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##   max_nu min_i int_Theta yi*phi(theta, xi) dnu(theta) subj. to nu in M(Theta) and ||nu||=1\n",
    "## = min_a max_nu sum_i int_{Theta+ U Theta-} ai yi*phitilde(theta, xi) dnu(theta) subj. to nu in P(Theta+ U Theta-) and a in P([N]), \n",
    "## where phitilde(theta, x) = phi(theta, x) if theta in Theta+ and -phi(theta, x) if theta in Theta- (cf Appdx A of Chizat21)\n",
    "# the network is parametrized such that the first m neurons are positively weighted and the last m are negatively weighted\n",
    "gfun(i, theta) = y[i] * activation(dot(theta, x[:,i]))\n",
    "\n",
    "function logit(x, w, theta)\n",
    "    out = 0\n",
    "    for j=1:m\n",
    "        out += w[j] * activation(dot(theta[:,j], x))\n",
    "    end\n",
    "    for j=m+1:2m\n",
    "        out += -w[j] * activation(dot(theta[:,j], x))\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "if bias\n",
    "    predict(x1, x2; w, theta, hard=true) = hard ? sign(logit([x1, x2, 1], w, theta)) : logit([x1, x2, 1], w, theta)\n",
    "else\n",
    "    predict(x1, x2; w, theta, hard=true) = hard ? sign(logit([x1, x2], w, theta)) : logit([x1, x2], w, theta)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(rng, 1234)\n",
    "# initialize adversary weights\n",
    "a = ones(N) ./ N\n",
    "# initialize network weights and positions nu=(w, theta); theta constrained to unit l2 sphere\n",
    "w = ones(2m) ./ (2m)\n",
    "theta = randn(rng, d, 2m)\n",
    "for j=1:2m\n",
    "    theta[:,j] ./= sqrt(sum(theta[:,j].^2))\n",
    "end\n",
    "\n",
    "# for extragradient step\n",
    "ap = similar(a)\n",
    "wp = similar(w)\n",
    "thetap = similar(theta)\n",
    "# to store intermediate values\n",
    "copies_a = Array{Float64}(undef, N, T+1)\n",
    "copies_w = Array{Float64}(undef, 2m, T+1)\n",
    "copies_theta = Array{Float64}(undef, d, 2m, T+1)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the decision region at random initialization https://discourse.julialang.org/t/plotting-decision-boundary-regions-for-classifier/21397\n",
    "contour(r, r, \n",
    "    (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "    f=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take a CP gradient step\n",
    "- starting from a, w, theta\n",
    "- evaluating the gradients at ap, wp, thetap\n",
    "- with stepsizes eta_a, eta_w, eta_theta\n",
    "(taking care of the fact that the network is parametrized such that the first m neurons are positively weighted and the last m are negatively weighted)\n",
    "Returns the updated particles a1, w1, theta1\n",
    "\"\"\"\n",
    "function step_CPMDA(\n",
    "        f,\n",
    "        a, w, theta,\n",
    "        ap, wp, thetap,\n",
    "        eta_a, eta_w, eta_theta;\n",
    "        true_prox=false,\n",
    "        constrain_theta=true\n",
    ")\n",
    "    N = length(a)\n",
    "    d = size(theta)[1]\n",
    "    m = Int(size(theta)[2] / 2)\n",
    "    \n",
    "    neuronsigns = ones(2m)\n",
    "    neuronsigns[m+1:2m] .= -1\n",
    "\n",
    "    Dfp = Array{Float64}(undef, d, N, 2m) # gradient of f w.r.t theta at thetap\n",
    "    for i=1:N, j=1:2m\n",
    "        Dfp[:,i,j] = ForwardDiff.gradient(tt -> f(i,tt), thetap[:,j])\n",
    "    end\n",
    "\n",
    "    # take step: adversary\n",
    "    s = Array{Float64}(undef, N)\n",
    "    for i=1:N\n",
    "        s[i] = sum( neuronsigns[j] * wp[j] * f(i, thetap[:,j]) for j=1:2m )\n",
    "    end\n",
    "    if eta_a == Inf\n",
    "        a1 = zeros(N)\n",
    "        a1[argmax(s)] = 1.\n",
    "    else\n",
    "        a1 = a .* exp.(-eta_a * s)\n",
    "        a1 ./= sum(a1)\n",
    "    end\n",
    "    \n",
    "    # take step: network\n",
    "    w1 = similar(w)\n",
    "    theta1 = similar(theta)\n",
    "    for j=1:2m\n",
    "        s = sum( ap[i] * f(i, thetap[:,j]) for i=1:N )\n",
    "        w1[j] = w[j] * exp(eta_w*neuronsigns[j]*s)\n",
    "    end\n",
    "    w1 ./= sum(w1)\n",
    "    for j=1:2m\n",
    "        s = sum( ap[i] * Dfp[:,i,j] for i=1:N )\n",
    "        @assert size(s) == (d,)\n",
    "        if true_prox\n",
    "            theta1[:,j] = theta[:,j] + eta_theta * neuronsigns[j] * wp[j] / w[j] * s\n",
    "        else\n",
    "            theta1[:,j] = theta[:,j] + eta_theta * neuronsigns[j] * s\n",
    "        end\n",
    "    end\n",
    "    # retract (just project) theta1 back to unit l2 sphere\n",
    "    if constrain_theta\n",
    "        for j=1:2m\n",
    "            theta1[:,j] ./= sqrt(sum(theta1[:,j].^2))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return a1, w1, theta1\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_a, eta_w, eta_theta = eta0_a, eta0_w, eta0_theta # constant stepsizes\n",
    "\n",
    "@showprogress 1 for t=1:T # minimum update interval of 1 second\n",
    "    copies_a[:,t] = copy(a)\n",
    "    copies_w[:,t] = copy(w)\n",
    "    copies_theta[:,:,t] = copy(theta)\n",
    "\n",
    "    # extragradient (\"ghost\" step)\n",
    "    # extrasteps=1: CP-MDA, extrasteps=2: CP-MP\n",
    "    ap, wp, thetap = a, w, theta\n",
    "    for s=1:extrasteps\n",
    "        ap, wp, thetap = step_CPMDA(    \n",
    "            gfun,\n",
    "            a, w, theta,\n",
    "            ap, wp, thetap,\n",
    "            eta_a, eta_w, eta_theta;\n",
    "            true_prox=true,\n",
    "            constrain_theta=constrain_theta\n",
    "        )\n",
    "    end\n",
    "\n",
    "    # take step\n",
    "    a, w, theta = ap, wp, thetap\n",
    "end\n",
    "\n",
    "copies_a[:,T+1] = copy(a)\n",
    "copies_w[:,T+1] = copy(w)\n",
    "copies_theta[:,:,T+1] = copy(theta)\n",
    "\n",
    "af, wf, thetaf = a, w, theta\n",
    "save(\"$resultdir/iterates.jld\", \"copies_a\", copies_a, \"copies_w\", copies_w, \"copies_theta\", copies_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the NI error\n",
    "\"\"\"\n",
    "Compute the \"global\" Nikaido-Isoda (NI) error\n",
    "    max_{a0, nu0} <a|F|nu0> - <a0|F|nu> = max_theta <a|F|delta_theta> - min_i <ei|F|nu>\n",
    "\"\"\"\n",
    "function glob_NI_err(gfun, a, w, theta; Ntheta=Int(1e4))\n",
    "    N = length(a)\n",
    "    d = size(theta)[1]\n",
    "    m = Int(size(theta)[2] / 2)\n",
    "    \n",
    "    if d==2\n",
    "        apcont = myCircle(Ntheta)\n",
    "    elseif d==3\n",
    "        apcont, Ntheta_new = mySphere(Ntheta)\n",
    "    else\n",
    "        error(\"glob_NI_err not implemented for d>3\")\n",
    "    end\n",
    "    maxtheta = -Inf\n",
    "    mintheta = +Inf\n",
    "    for k=1:Ntheta_new\n",
    "        theta0 = apcont[:,k]\n",
    "        s = 0\n",
    "        for i=1:N\n",
    "            s += a[i] * gfun(i, theta0)\n",
    "        end\n",
    "        maxtheta = max(maxtheta, s)\n",
    "        mintheta = min(mintheta, s)\n",
    "    end\n",
    "    Fnu(i) = sum( w[j] * gfun(i, theta[:,j]) for j=1:m ) - sum( w[j] * gfun(i, theta[:,j]) for j=m+1:2m )\n",
    "    mini = minimum(Fnu(i) for i=1:N)\n",
    "    return max(maxtheta, -mintheta) - mini\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nierrs = Array{Float64}(undef, T+2)\n",
    "@showprogress 1 for t=TNI_min:evalNIevery:TNI_max+1 # minimum update interval of 1 second\n",
    "# for t=TNI_min:evalNIevery:TNI_max+1\n",
    "    nierrs[t] = glob_NI_err(gfun, copies_a[:,t], copies_w[:,t], copies_theta[:,:,t]; Ntheta=Ntheta)\n",
    "end\n",
    "if !skip_avg\n",
    "    nierrs[T+2] = glob_NI_err(gfun, avg_a, avg_w, avg_theta; Ntheta=Ntheta)\n",
    "end\n",
    "open(logfile, \"a\") do f\n",
    "    for t=TNI_min:evalNIevery:TNI_max+1\n",
    "        write(f, \"glob_NI_err at iteration#$t: $(nierrs[t])\\n\")\n",
    "    end\n",
    "    if !skip_avg \n",
    "        write(f, \"glob_NI_err at avg iterate: $(nierrs[T+2])\\n\") \n",
    "    end\n",
    "end\n",
    "save(\"$resultdir/nierrs__every$(evalNIevery)__t=$(TNI_min)--$(TNI_max).jld\", \"nierrs\", nierrs)\n",
    "\n",
    "plt_NI = plot(range(TNI_min, stop=TNI_max+1, step=evalNIevery), nierrs[TNI_min:evalNIevery:TNI_max+1], xlabel=\"k\", label=\"\")\n",
    "if !skip_avg\n",
    "    hline!([nierrs[T+2]], label=\"avg iterate\")\n",
    "end\n",
    "if !hidetitle\n",
    "    title!(\"NI error of iterates\")\n",
    "end\n",
    "fn = \"$resultdir/NI_errors.png\"\n",
    "savefig(plt_NI, fn)\n",
    "\n",
    "eps = 1e-10 # numerical stability (we use approximations (with deltax, deltay) to compute glob_NI_err)\n",
    "plt_NI_log = plot(range(TNI_min, stop=TNI_max+1, step=evalNIevery), eps .+ max.(0, nierrs[TNI_min:evalNIevery:(TNI_max+1)]), xlabel=\"k\", label=\"\", yscale=:log10)\n",
    "if !skip_avg\n",
    "    hline!([ eps + max(0, nierrs[T+2]) ], label=(hidelabels ? \"\" : \"avg iterate\"))\n",
    "end\n",
    "if !hidetitle\n",
    "    title!(\"NI error of iterates (log-linear scale)\")\n",
    "end\n",
    "fn = \"$resultdir/NI_errors_logscale.png\"\n",
    "savefig(plt_NI_log, fn)\n",
    "\n",
    "plt_NI_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decision region at the last iterate\n",
    "pltt = contour(r, r,\n",
    "    (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "    f=true)\n",
    "contour!(r, r,\n",
    "    (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "    levels=[0.],\n",
    "    seriescolor=:blues,\n",
    "    linestyle=:dash,\n",
    "    linewidth=3)\n",
    "scatter!(x[1, y.==1], x[2, y.==1], m=:circ, markersize=8, label=\"+\", color=:green)  \n",
    "scatter!(x[1, y.==-1], x[2, y.==-1], m=:utriangle, markersize=8, label=\"-\", color=:red)\n",
    "\n",
    "fn = \"$resultdir/contour_soft_lastiter.png\"\n",
    "savefig(pltt, fn)\n",
    "pltt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the neurons (are they sparsely concentrated?)\n",
    "function plot_neurons_2d(w, theta; resultdir=nothing)\n",
    "    d = size(theta)[1]\n",
    "    m = Int(length(w)/2)\n",
    "    @assert d==2\n",
    "    r0 = 1/m\n",
    "    circle = r0 .* myCircle(500)\n",
    "    plt = plot(circle[1,:], circle[2,:], aspect_ratio=1.0, label=false)\n",
    "    neur = zeros(2, 2m)\n",
    "    for j=1:2m\n",
    "        neur[:,j] = w[j] * theta[:,j]\n",
    "    end\n",
    "    for j=1:m\n",
    "        plot!([0, neur[1,j]], [0, neur[2,j]], color=:red, label=false)\n",
    "    end\n",
    "    for j=m+1:2m\n",
    "        plot!([0, neur[1,j]], [0, neur[2,j]], color=:blue, label=false)\n",
    "    end\n",
    "    scatter!(neur[1,1:m],    neur[2,1:m],    markersize=4, markercolor=:red, label=false)\n",
    "    scatter!(neur[1,m+1:2m], neur[2,m+1:2m], markersize=4, markercolor=:blue, label=false)\n",
    "    if !isnothing(resultdir)\n",
    "        fn = \"$resultdir/neurons_lastiter.png\"\n",
    "        savefig(plt, fn)\n",
    "    end\n",
    "    return plt\n",
    "end\n",
    "\n",
    "function plot_neurons_fake3d(w, theta; hidetitle=false, resultdir=nothing)\n",
    "    plt_e1 = plot_neurons_2d(w, theta[ [2,3], :])\n",
    "    xlabel!(\"theta_2\")\n",
    "    ylabel!(\"theta_3\")\n",
    "    if !hidetitle\n",
    "        title!(\"neurons projected along axis theta_1\")\n",
    "    end\n",
    "    plt_e2 = plot_neurons_2d(w, theta[ [1,3], :])\n",
    "    xlabel!(\"theta_1\")\n",
    "    ylabel!(\"theta_3\")\n",
    "    if !hidetitle\n",
    "        title!(\"neurons projected along axis theta_2\")\n",
    "    end\n",
    "    plt_e3 = plot_neurons_2d(w, theta[ [1,2], :])\n",
    "    xlabel!(\"theta_1\")\n",
    "    ylabel!(\"theta_2\")\n",
    "    if !hidetitle\n",
    "        title!(\"neurons projected along axis theta_3\")\n",
    "    end\n",
    "    plt_combined = plot(plt_e1, plt_e2, plt_e3, layout=(1, 3), size=(1500, 500))\n",
    "    if !isnothing(resultdir)\n",
    "        fn = \"$resultdir/neurons_lastiter_projtheta1.png\"\n",
    "        savefig(plt_e1, fn)\n",
    "        fn = \"$resultdir/neurons_lastiter_projtheta2.png\"\n",
    "        savefig(plt_e2, fn)\n",
    "        fn = \"$resultdir/neurons_lastiter_projtheta3.png\"\n",
    "        savefig(plt_e3, fn)\n",
    "        fn = \"$resultdir/neurons_lastiter.png\"\n",
    "        savefig(plt_combined, fn)\n",
    "    end\n",
    "    plt_e1, plt_e2, plt_e3, plt_combined\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## neurons at last iterate\n",
    "if d==3\n",
    "    plt_e1, plt_e2, plt_e3, plt_combined = plot_neurons_fake3d(copies_w[:,T+1], copies_theta[:,:,T+1];\n",
    "            hidetitle=hidetitle, resultdir=resultdir)\n",
    "elseif d==2\n",
    "    plt_combined = plot_neurons_2d(copies_w[:,T+1], copies_theta[:,:,T+1];\n",
    "            resultdir=resultdir)\n",
    "end\n",
    "plt_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copies_w[:,T+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the decision region across iterations (gif)\n",
    "if !skip_gifs\n",
    "    p = Progress(length(Tplotreg_min:plotregevery:Tplotreg_max))\n",
    "\n",
    "    anim = @animate for t=Tplotreg_min:plotregevery:Tplotreg_max\n",
    "        contour(r, r,\n",
    "            (x1, x2) -> predict(x1, x2; w=copies_w[:,t], theta=copies_theta[:,:,t], hard=false),\n",
    "            f=true)\n",
    "        contour!(r, r,\n",
    "            (x1, x2) -> predict(x1, x2; w=copies_w[:,t], theta=copies_theta[:,:,t], hard=false),\n",
    "            levels=[0.],\n",
    "            seriescolor=:blues,\n",
    "            linestyle=:dash,\n",
    "            linewidth=3)\n",
    "        scatter!(x[1, y.==1], x[2, y.==1], m=:circ, markersize=8, label=\"+\", color=:green)  \n",
    "        scatter!(x[1, y.==-1], x[2, y.==-1], m=:utriangle, markersize=8, label=\"-\", color=:red)\n",
    "        title!(\"Iteration $t\")\n",
    "        next!(p)\n",
    "    end\n",
    "\n",
    "    fn = \"$resultdir/contour_soft__every$(plotregevery)__$Tplotreg_min-$Tplotreg_max.gif\"\n",
    "    gif(anim, fn, fps=10)\n",
    "\n",
    "    cp(anim.dir, resultdir*\"/frames_contour_soft__every$(plotregevery)__$Tplotreg_min-$Tplotreg_max\", force=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
