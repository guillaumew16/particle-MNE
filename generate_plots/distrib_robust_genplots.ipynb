{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Logging\n",
    "logger = ConsoleLogger(stdout)\n",
    "# debuglogger = ConsoleLogger(stderr, Logging.Debug)\n",
    "global_logger(logger)\n",
    "using JLD\n",
    "\n",
    "using Random\n",
    "rng = MersenneTwister(1234)\n",
    "import Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "pyplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "using ProgressMeter\n",
    "using LinearAlgebra: dot\n",
    "include(\"../utils/misc.jl\")\n",
    "using .MiscUtils: myCircle, mySphere, myDisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = Dates.now()\n",
    "\n",
    "## model parameters\n",
    "input_dim = 2 # input dim\n",
    "bias = true\n",
    "m = 50  # nb hidden neurons for each sign\n",
    "N = 5   # nb samples\n",
    "n = 10  # nb particles per sample\n",
    "rob = 0.2 # robustness level\n",
    "# rob = 0.3\n",
    "activation_str = \"ReLUcub\"\n",
    "\n",
    "## logging\n",
    "ts_fsfriendly = Dates.format(ts, \"yyyy-mm-ddTHHMMSS\") # filesystem-friendly string for ts\n",
    "resultdir = mkpath(\"../results/DR__dim$(input_dim)bias$(bias)__m$(m)__N$(N)__n$(n)__rob$(rob)__$(activation_str)__$(ts_fsfriendly)\")\n",
    "\n",
    "logfile = \"$resultdir/log.txt\"\n",
    "touch(logfile)\n",
    "open(logfile, \"a\") do f\n",
    "    write(f, \"input_dim=$input_dim\\n\")\n",
    "    write(f, \"bias: $bias\\n\")\n",
    "    write(f, \"m=$m\\n\")\n",
    "    write(f, \"N=$N\\n\")\n",
    "    write(f, \"n=$n\\n\")\n",
    "    write(f, \"rob=$rob\\n\")\n",
    "    write(f, \"activation: $activation_str\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## algo parameters\n",
    "if rob == 0.2\n",
    "    T = 500\n",
    "    eta0_a = 8e-1 # initial stepsize\n",
    "    eta0_u = 8e-3\n",
    "    eta0_w = 8e-1\n",
    "    eta0_theta = 8e-2\n",
    "elseif rob == 0.3\n",
    "    T = 1500\n",
    "    eta0_a = 6e-1 # initial stepsize\n",
    "    eta0_u = 6e-3\n",
    "    eta0_w = 6e-1\n",
    "    eta0_theta = 6e-2\n",
    "end\n",
    "constrain_theta = true\n",
    "extrasteps = 2 # extrasteps=1: CP-MDA, extrasteps=2: CP-MP\n",
    "\n",
    "open(logfile, \"a\") do f\n",
    "    write(f, \"T=$T\\n\")\n",
    "    write(f, \"eta0_a=$(eta0_a)\\n\")\n",
    "    write(f, \"eta0_u=$(eta0_u)\\n\")\n",
    "    write(f, \"eta0_w=$(eta0_w)\\n\")\n",
    "    write(f, \"eta0_theta=$(eta0_theta)\\n\")\n",
    "    write(f, \"constrain_theta: $(constrain_theta)\\n\")\n",
    "    write(f, \"extrasteps=$(extrasteps)\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting parameters\n",
    "if rob == 0.2\n",
    "    TNI_min = 1\n",
    "    TNI_max = 400\n",
    "    evalNIevery = Int(floor((TNI_max-TNI_min)/200))\n",
    "    Ntheta = Int(1e5)\n",
    "    Nu = Int(1e4)\n",
    "elseif rob == 0.3\n",
    "    TNI_min = 1\n",
    "    TNI_max = 1200\n",
    "    evalNIevery = Int(floor((TNI_max-TNI_min)/200))\n",
    "    Ntheta = Int(1e5)\n",
    "    Nu = Int(1e4)\n",
    "end\n",
    "skip_avg = true # avg not implemented\n",
    "\n",
    "@assert input_dim==2\n",
    "# r = range(-3, 3, length=101)\n",
    "r = range(-2.2, 2.2, length=101)\n",
    "\n",
    "Tplotreg_min = 1 # plot decision regions gif\n",
    "Tplotreg_max = T\n",
    "plotregevery = Int(floor((Tplotreg_max-Tplotreg_min)/50))\n",
    "hidetitle = true\n",
    "hidelabel = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if activation_str == \"ReLU\"\n",
    "    activation(x) = max(0, x)\n",
    "elseif activation_str == \"abs\"\n",
    "    activation(x) = abs(x)\n",
    "elseif activation_str == \"ReLUsq\"\n",
    "    activation(x) = max(0, x)^2\n",
    "elseif activation_str == \"ReLUcub\"\n",
    "    activation(x) = max(0, x)^3\n",
    "elseif activation_str == \"ReLUquar\"\n",
    "    activation(x) = max(0, x)^4\n",
    "# elseif activation_str == \"sq\" # polynomial activation is silly\n",
    "#     activation(x) = x^2\n",
    "elseif activation_str == \"sigmoid\"\n",
    "    activation(x) = 1/(1+exp(-x))\n",
    "elseif activation_str == \"tanh\"\n",
    "    activation(x) = tanh(x)\n",
    "else\n",
    "    error(\"activation_str not recognized, should be \n",
    "        \\\"ReLU\\\" or \n",
    "        \\\"abs\\\" or \n",
    "        \\\"ReLUsq\\\" or \n",
    "        \\\"ReLUcub\\\" or \n",
    "        \\\"ReLUquar\\\" or \n",
    "        \\\"sigmoid\\\" \n",
    "        or \\\"tanh\\\"\")\n",
    "end\n",
    "\n",
    "if bias == true\n",
    "    d = input_dim + 1\n",
    "else\n",
    "    d = input_dim\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function makedataset(d, N; bias, rng=MersenneTwister(1234))\n",
    "    x = randn(rng, d, N)\n",
    "    if bias\n",
    "        x[d,:] .= 1\n",
    "    end\n",
    "    y = sign.(rand(rng, N).-0.5) # -1 or 1 uniformly\n",
    "    return x, y\n",
    "end\n",
    "\n",
    "Random.seed!(rng, 1234)\n",
    "x, y = makedataset(d, N; bias=bias, rng=rng)\n",
    "save(\"$resultdir/dataset.jld\", \"x\", x, \"y\", y)\n",
    "\n",
    "plt = scatter(x[1, y.==1], x[2, y.==1], m=:circ, markersize=8, label=\"+\", color=:green)\n",
    "scatter!(x[1, y.==-1], x[2, y.==-1], m=:utriangle, markersize=8, label=\"-\", color=:red)\n",
    "for k=1:N\n",
    "    circle = x[1:2, k] .+ rob .* myCircle(500)\n",
    "    plot!(circle[1,:], circle[2,:], aspect_ratio=1.0, label=false, color=(y[k]==1 ? :green : :red))\n",
    "end\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## min-max objective: obj(a, u, b, theta) = sum_{k=1}^N sum_{i=1}^n sum_{j=1}^m a_{ki} b_j neuronsigns[j] y_k sigma(theta_j.T (x_k+u_{ki}))\n",
    "if bias\n",
    "    # gfun(u, theta, k) = y[k] * activation(dot(theta[1:input_dim], x[1:input_dim,k] .+ u) + theta[d]*x[d,k]) # for some reason ForwardDiff doesn't like this\n",
    "    gfun(u, theta, k) = y[k] * activation(dot(theta[1:2], x[1:2,k] .+ u) + theta[d]*x[d,k])\n",
    "else\n",
    "    gfun(u, theta, k) = y[k] * activation(dot(theta, x[:,k] .+ u))\n",
    "end\n",
    "\n",
    "function logit(x, w, theta)\n",
    "    out = 0\n",
    "    for j=1:m\n",
    "        out += w[j] * activation(dot(theta[:,j], x))\n",
    "    end\n",
    "    for j=m+1:2m\n",
    "        out += -w[j] * activation(dot(theta[:,j], x))\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "if bias\n",
    "    predict(x1, x2; w, theta, hard=true) = hard ? sign(logit([x1, x2, 1], w, theta)) : logit([x1, x2, 1], w, theta)\n",
    "else\n",
    "    predict(x1, x2; w, theta, hard=true) = hard ? sign(logit([x1, x2], w, theta)) : logit([x1, x2], w, theta)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(rng, 1234)\n",
    "# initialize adversary weights\n",
    "a = ones(N, n) ./ (n*N)\n",
    "u = randn(rng, input_dim, N, n) # (and not d, in case bias=true)\n",
    "for k=1:N, i=1:n\n",
    "    u[:,k,i] ./= sqrt(sum(u[:,k,i].^2))\n",
    "    u[:,k,i] .*= rob * rand(rng)\n",
    "end\n",
    "# initialize network weights and positions nu=(w, theta); theta constrained to unit l2 sphere\n",
    "w = ones(2m) ./ (2m)\n",
    "theta = randn(rng, d, 2m)\n",
    "for j=1:2m\n",
    "    theta[:,j] ./= sqrt(sum(theta[:,j].^2))\n",
    "end\n",
    "\n",
    "# for extragradient step\n",
    "ap = similar(a)\n",
    "up = similar(u)\n",
    "wp = similar(w)\n",
    "thetap = similar(theta)\n",
    "# to store intermediate values\n",
    "copies_a = Array{Float64}(undef, N, n, T+1)\n",
    "copies_u = Array{Float64}(undef, input_dim, N, n, T+1)\n",
    "copies_w = Array{Float64}(undef, 2m, T+1)\n",
    "copies_theta = Array{Float64}(undef, d, 2m, T+1)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the decision region at random initialization https://discourse.julialang.org/t/plotting-decision-boundary-regions-for-classifier/21397\n",
    "plt = contour(r, r, \n",
    "    (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "    f=true)\n",
    "scatter!(x[1, y.==1], x[2, y.==1], m=:circ, markersize=8, label=\"+\", color=:green)\n",
    "scatter!(x[1, y.==-1], x[2, y.==-1], m=:utriangle, markersize=8, label=\"-\", color=:red)\n",
    "for k=1:N\n",
    "    circle = x[1:2, k] .+ rob .* myCircle(500)\n",
    "    plot!(circle[1,:], circle[2,:], #aspect_ratio=1.0, \n",
    "        label=false, color=(y[k]==1 ? :green : :red))\n",
    "    scatter!(x[1,k].+u[1,k,:], x[2,k].+u[2,k,:], alpha=a[k,:], markersize=3, markerstrokewidth=0, label=false, color=(y[k]==1 ? :green : :red))\n",
    "end\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take a CP gradient step\n",
    "- starting from a, u, w, theta\n",
    "- evaluating the gradients at ap, up, wp, thetap\n",
    "- with stepsizes eta_a, eta_u, eta_w, eta_theta\n",
    "(taking care of the fact that the network is parametrized such that the first m neurons are positively weighted and the last m are negatively weighted)\n",
    "Returns the updated particles a1, u1, w1, theta1\n",
    "\"\"\"\n",
    "function step_CPMDA(\n",
    "        f, rob,\n",
    "        a, u, w, theta,\n",
    "        ap, up, wp, thetap,\n",
    "        eta_a, eta_u, eta_w, eta_theta;\n",
    "        true_prox=false,\n",
    "        constrain_theta=true\n",
    ")\n",
    "    N, n = size(a)\n",
    "    d = size(theta)[1]\n",
    "    m = Int(size(theta)[2] / 2)\n",
    "    \n",
    "    neuronsigns = ones(2m)\n",
    "    neuronsigns[m+1:2m] .= -1\n",
    "\n",
    "    Dtheta_fp = Array{Float64}(undef, d, N, n, 2m) # gradient of f w.r.t theta at thetap\n",
    "    for k=1:N, i=1:n, j=1:2m\n",
    "        Dtheta_fp[:,k,i,j] = ForwardDiff.gradient(tt -> f(up[:,k,i], tt, k), thetap[:,j])\n",
    "    end\n",
    "    Du_fp = Array{Float64}(undef, input_dim, N, n, 2m) # gradient of f w.r.t u at up\n",
    "    for k=1:N, i=1:n, j=1:2m\n",
    "        Du_fp[:,k,i,j] = ForwardDiff.gradient(uu -> f(uu, thetap[:,j], k), up[:,k,i])\n",
    "    end\n",
    "\n",
    "    # take step: adversary\n",
    "    a1 = similar(a)\n",
    "    u1 = similar(u)\n",
    "    for k=1:N, i=1:n\n",
    "        s = sum( neuronsigns[j] * wp[j] * f(up[:,k,i], thetap[:,j], k) for j=1:2m )\n",
    "        a1[k,i] = a[k,i] * exp(-eta_a * s)\n",
    "    end\n",
    "    a1 ./= sum(a1)\n",
    "    for k=1:N, i=1:n\n",
    "        s = sum( neuronsigns[j] * wp[j] * Du_fp[:,k,i,j] for j=1:2m)\n",
    "        @assert size(s) == (input_dim,)\n",
    "        if true_prox\n",
    "            u1[:,k,i] = u[:,k,i] - eta_u * ap[k,i] / a[k,i] * s\n",
    "        else\n",
    "            u1[:,k,i] = u[:,k,i] - eta_u * s\n",
    "        end\n",
    "    end\n",
    "    # project u1 back to l2 ball of radius rob\n",
    "    for k=1:N, i=1:n\n",
    "        uki_sqnorm = sum(u1[:,k,i].^2)\n",
    "        if uki_sqnorm > rob^2\n",
    "            u1[:,k,i] ./= sqrt(uki_sqnorm)\n",
    "            u1[:,k,i] .*= rob\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # take step: network\n",
    "    w1 = similar(w)\n",
    "    theta1 = similar(theta)\n",
    "    for j=1:2m\n",
    "        s = sum( ap[k,i] * f(up[:,k,i], thetap[:,j], k) for k=1:N, i=1:n )\n",
    "        w1[j] = w[j] * exp(eta_w*neuronsigns[j]*s)\n",
    "    end\n",
    "    w1 ./= sum(w1)\n",
    "    for j=1:2m\n",
    "        s = sum( ap[k,i] * Dtheta_fp[:,k,i,j] for k=1:N, i=1:n )\n",
    "        @assert size(s) == (d,)\n",
    "        if true_prox\n",
    "            theta1[:,j] = theta[:,j] + eta_theta * neuronsigns[j] * wp[j] / w[j] * s\n",
    "        else\n",
    "            theta1[:,j] = theta[:,j] + eta_theta * neuronsigns[j] * s\n",
    "        end\n",
    "    end\n",
    "    # retract (just project) theta1 back to unit l2 sphere\n",
    "    if constrain_theta\n",
    "        for j=1:2m\n",
    "            theta1[:,j] ./= sqrt(sum(theta1[:,j].^2))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return a1, u1, w1, theta1\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_a, eta_u, eta_w, eta_theta = eta0_a, eta0_u, eta0_w, eta0_theta # constant stepsizes\n",
    "\n",
    "@showprogress 1 for t=1:T # minimum update interval of 1 second\n",
    "    copies_a[:,:,t] = copy(a)\n",
    "    copies_u[:,:,:,t] = copy(u)\n",
    "    copies_w[:,t] = copy(w)\n",
    "    copies_theta[:,:,t] = copy(theta)\n",
    "\n",
    "    # extragradient (\"ghost\" step)\n",
    "    # extrasteps=1: CP-MDA, extrasteps=2: CP-MP\n",
    "    ap, up, wp, thetap = a, u, w, theta\n",
    "    for s=1:extrasteps\n",
    "        ap, up, wp, thetap = step_CPMDA(\n",
    "            gfun, rob,\n",
    "            a, u, w, theta,\n",
    "            ap, up, wp, thetap,\n",
    "            eta_a, eta_u, eta_w, eta_theta;\n",
    "            true_prox=true,\n",
    "            constrain_theta=constrain_theta\n",
    "        )\n",
    "    end\n",
    "\n",
    "    # take step\n",
    "    a, u, w, theta = ap, up, wp, thetap\n",
    "end\n",
    "\n",
    "copies_a[:,:,T+1] = copy(a)\n",
    "copies_u[:,:,:,T+1] = copy(u)\n",
    "copies_w[:,T+1] = copy(w)\n",
    "copies_theta[:,:,T+1] = copy(theta)\n",
    "\n",
    "af, uf, wf, thetaf = a, u, w, theta\n",
    "save(\"$resultdir/iterates.jld\", \"copies_a\", copies_a, \"copies_w\", copies_w, \"copies_theta\", copies_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(af, dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the NI error\n",
    "\"\"\"\n",
    "Compute the \"global\" Nikaido-Isoda (NI) error\n",
    "    max_{a0, nu0} <a|F|nu0> - <a0|F|nu> = max_theta <a|F|delta_theta> - min_i <ei|F|nu>\n",
    "\"\"\"\n",
    "function glob_NI_err(gfun, rob, a, u, w, theta; Ntheta=Int(1e4), Nu=Int(1e2), apcont_theta=nothing, apcont_nu=nothing)\n",
    "    N, n = size(a)\n",
    "    input_dim = size(u)[1]\n",
    "    d = size(theta)[1]\n",
    "    m = Int(size(theta)[2] / 2)\n",
    "\n",
    "    neuronsigns = ones(2m)\n",
    "    neuronsigns[m+1:2m] .= -1\n",
    "    \n",
    "    if !isnothing(apcont_theta)\n",
    "        Ntheta_new = size(apcont_theta)[2]\n",
    "    else\n",
    "        if d==2\n",
    "            apcont_theta = myCircle(Ntheta)\n",
    "        elseif d==3\n",
    "            apcont_theta, Ntheta_new = mySphere(Ntheta)\n",
    "        else\n",
    "            error(\"glob_NI_err not implemented for d>3\")\n",
    "        end\n",
    "    end\n",
    "    maxtheta = -Inf\n",
    "    mintheta = +Inf\n",
    "    for l=1:Ntheta_new\n",
    "        theta0 = apcont_theta[:,l]\n",
    "        s = sum( a[k,i] * gfun(u[:,k,i], theta0, k) for k=1:N, i=1:n )\n",
    "        maxtheta = max(maxtheta, s)\n",
    "        mintheta = min(mintheta, s)\n",
    "    end\n",
    "\n",
    "    if !isnothing(apcont_nu)\n",
    "        Nu_new = size(apcont_nu)[2]\n",
    "    else\n",
    "        @assert input_dim==2\n",
    "        apcont_nu, Nu_new = myDisk(Nu)\n",
    "        apcont_nu .*= rob\n",
    "    end\n",
    "    minu = Inf\n",
    "    for k=1:N # putting all mass on x_k + u for some u in B_{0,rob}\n",
    "        for l=1:Nu_new\n",
    "            u0 = apcont_nu[:,l]\n",
    "            s = sum( neuronsigns[j] * w[j] * gfun(u0, theta[:,j], k) for j=1:2m )\n",
    "            minu = min(minu, s)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return max(maxtheta, -mintheta) - minu\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## rob=0.2\n",
    "# TNI_min = 1\n",
    "# TNI_max = 400\n",
    "# evalNIevery = Int(floor((TNI_max-TNI_min)/200))\n",
    "# Ntheta = Int(1e5)\n",
    "# Nu = Int(1e4)\n",
    "\n",
    "# ## rob=0.3\n",
    "# TNI_min = 1\n",
    "# TNI_max = 1200\n",
    "# evalNIevery = Int(floor((TNI_max-TNI_min)/200))\n",
    "# Ntheta = Int(1e5)\n",
    "# Nu = Int(1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nierrs = Array{Float64}(undef, T+2)\n",
    "\n",
    "if d==2\n",
    "    apcont_theta = myCircle(Ntheta)\n",
    "elseif d==3\n",
    "    apcont_theta, Ntheta_new = mySphere(Ntheta)\n",
    "else\n",
    "    error(\"glob_NI_err not implemented for d>3\")\n",
    "end\n",
    "@assert input_dim==2\n",
    "apcont_nu, Nu_new = myDisk(Nu)\n",
    "apcont_nu .*= rob\n",
    "\n",
    "@showprogress 1 for t=TNI_min:evalNIevery:TNI_max+1 # minimum update interval of 1 second\n",
    "# for t=TNI_min:evalNIevery:TNI_max+1\n",
    "    nierrs[t] = glob_NI_err(gfun, rob, copies_a[:,:,t], copies_u[:,:,:,t], copies_w[:,t], copies_theta[:,:,t]; apcont_theta=apcont_theta, apcont_nu=apcont_nu) # Ntheta=Ntheta, Nu=Nu)\n",
    "end\n",
    "if !skip_avg\n",
    "    nierrs[T+2] = glob_NI_err(gfun, rob, avg_a, avg_u, avg_w, avg_theta; Ntheta=Ntheta)\n",
    "end\n",
    "open(logfile, \"a\") do f\n",
    "    for t=TNI_min:evalNIevery:TNI_max+1\n",
    "        write(f, \"glob_NI_err at iteration#$t: $(nierrs[t])\\n\")\n",
    "    end\n",
    "    if !skip_avg \n",
    "        write(f, \"glob_NI_err at avg iterate: $(nierrs[T+2])\\n\") \n",
    "    end\n",
    "end\n",
    "save(\"$resultdir/nierrs__every$(evalNIevery)__t=$(TNI_min)--$(TNI_max).jld\", \"nierrs\", nierrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10 # numerical stability (we use approximations (with deltax, deltay) to compute glob_NI_err)\n",
    "upscale=2   # adjusting fontsize, linewidths etc.\n",
    "resscale=2  # keep everything else fixed but increase resolution\n",
    "fontsize=11/upscale*1.75\n",
    "plt_NI_log = plot(range(TNI_min, stop=TNI_max+1, step=evalNIevery), eps .+ max.(0, nierrs[TNI_min:evalNIevery:(TNI_max+1)]), xlabel=\"k\", label=\"\", yscale=:log10,\n",
    "    linewidth=upscale,\n",
    "    xtickfontsize=fontsize, ytickfontsize=fontsize, xguidefontsize=fontsize, yguidefontsize=fontsize, legendfontsize=fontsize,\n",
    "    dpi=upscale*100*resscale,\n",
    "    size=(600/upscale, 400/upscale))\n",
    "if !skip_avg\n",
    "    hline!([ eps + max(0, nierrs[T+2]) ], label=(hidelabels ? \"\" : \"avg iterate\"))\n",
    "end\n",
    "if !hidetitle\n",
    "    title!(\"NI error of iterates (log-linear scale)\")\n",
    "end\n",
    "fn = \"$resultdir/NI_errors_logscale.png\"\n",
    "savefig(plt_NI_log, fn)\n",
    "plt_NI_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## decision region at the last iterate\n",
    "# pltt = contour(r, r,\n",
    "#     (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "#     f=true)\n",
    "# contour!(r, r,\n",
    "#     (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "#     levels=[0.],\n",
    "#     seriescolor=:blues,\n",
    "#     linestyle=:dash,\n",
    "#     linewidth=3)\n",
    "# scatter!(x[1, y.==1], x[2, y.==1], m=:circ, markersize=8, label=\"+\", color=:green)\n",
    "# scatter!(x[1, y.==-1], x[2, y.==-1], m=:utriangle, markersize=8, label=\"-\", color=:red)\n",
    "# for k=1:N\n",
    "#     circle = x[1:2, k] .+ rob .* myCircle(500)\n",
    "#     plot!(circle[1,:], circle[2,:], #aspect_ratio=1.0, \n",
    "#         label=false, color=(y[k]==1 ? :green : :red))\n",
    "#     scatter!(x[1,k].+u[1,k,:], x[2,k].+u[2,k,:], \n",
    "#         # alpha=a[k,:], \n",
    "#         alpha=1,\n",
    "#         markersize=4, markerstrokewidth=0, label=false, color=(y[k]==1 ? :green : :red))\n",
    "# end\n",
    "\n",
    "# fn = \"$resultdir/contour_soft_lastiter_withsatellites.png\"\n",
    "# savefig(pltt, fn)\n",
    "# pltt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decision region at the last iterate\n",
    "\n",
    "upscale=2   # adjusting fontsize, linewidths etc.\n",
    "resscale=2  # keep everything else fixed but increase resolution\n",
    "fontsize=11/upscale*1.75\n",
    "\n",
    "pltt = contour(r, r,\n",
    "    # (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "    (x1, x2) -> max(-6, predict(x1, x2; w=w, theta=theta, hard=false) ),\n",
    "    f=true,\n",
    "    levels=-6:1.:6,\n",
    "    clims=(-6, 6),\n",
    "    linewidth=upscale,\n",
    "    xtickfontsize=fontsize, ytickfontsize=fontsize, xguidefontsize=fontsize, yguidefontsize=fontsize, legendfontsize=fontsize*0.8,\n",
    "    dpi=upscale*100*resscale,\n",
    "    size=(600/upscale, 500/upscale))\n",
    "contour!(r, r,\n",
    "    # (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "    (x1, x2) -> max(-6, predict(x1, x2; w=w, theta=theta, hard=false) ),\n",
    "    levels=[0.],\n",
    "    seriescolor=:blues,\n",
    "    linestyle=:dash,\n",
    "    linewidth=3/upscale)\n",
    "scatter!(x[1, y.==1], x[2, y.==1], m=:circ, markersize=8/upscale, markerstrokewidth=1/upscale, label=\"+\", color=:green)\n",
    "scatter!(x[1, y.==-1], x[2, y.==-1], m=:utriangle, markersize=8/upscale, markerstrokewidth=1/upscale, label=\" -\", color=:red)\n",
    "for k=1:N\n",
    "    circle = x[1:2, k] .+ rob .* myCircle(500)\n",
    "    plot!(circle[1,:], circle[2,:], #aspect_ratio=1.0, \n",
    "        label=false, color=(y[k]==1 ? :green : :red))\n",
    "    scatter!(x[1,k].+u[1,k,:], x[2,k].+u[2,k,:], \n",
    "        # alpha=a[k,:], \n",
    "        alpha=1,\n",
    "        markersize=6/upscale, markerstrokewidth=1/upscale, label=false, color=(y[k]==1 ? :darkgreen : :darkred))\n",
    "end\n",
    "\n",
    "fn = \"$resultdir/contour_soft_lastiter_withsatellites.png\"\n",
    "savefig(pltt, fn)\n",
    "pltt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots.PlotMeasures # enable setting \"margins=10mm\" in plots\n",
    "using LaTeXStrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscale=2   # adjusting fontsize, linewidths etc.\n",
    "resscale=2  # keep everything else fixed but increase resolution\n",
    "fontsize=11/upscale*1.4\n",
    "\n",
    "azimuth = 25\n",
    "elevation = 55\n",
    "\n",
    "phi_s = range(0, 2pi, length=100)\n",
    "theta_s = range(0, pi, length=100)\n",
    "r0 = 1/m\n",
    "xss = r0 .* [cos(t)*sin(p) for t in theta_s, p in phi_s]\n",
    "yss = r0 .* [sin(t)*sin(p) for t in theta_s, p in phi_s]\n",
    "zss = r0 .* [cos(p) for t in theta_s, p in phi_s]\n",
    "pltt = plot(xss, yss, zss, linetype=:surface, colorbar=false, seriescolor=:greens, alpha=0.1, \n",
    "    xlabel=L\"θ_1\", ylabel=L\"θ_2\", zlabel=L\"θ_3\",\n",
    "    xlim=(-0.05, 0.05),\n",
    "    ylim=(-0.05, 0.05),\n",
    "    zlim=(-0.05, 0.05),\n",
    "    camera = (azimuth, elevation),\n",
    "    margins=10mm,\n",
    "    xtickfontsize=0.6*fontsize, ytickfontsize=0.6*fontsize, ztickfontsize=0.6*fontsize,\n",
    "    xguidefontsize=fontsize, yguidefontsize=fontsize, zguidefontsize=fontsize,\n",
    "    legendfontsize=fontsize*0.8,\n",
    "    dpi=upscale*100*resscale,\n",
    "    size=(600/upscale, 600/upscale),\n",
    "    aspect_ratio=:equal)\n",
    "theta = thetaf\n",
    "neur = zeros(3, 2m)\n",
    "for j=1:2m\n",
    "    neur[:,j] = w[j] * theta[:,j]\n",
    "end\n",
    "for j=1:m\n",
    "    plot!([0, neur[1,j]], [0, neur[2,j]], [0, neur[3,j]], color=:red, label=false)\n",
    "end\n",
    "for j=m+1:2m\n",
    "    plot!([0, neur[1,j]], [0, neur[2,j]], [0, neur[3,j]], color=:blue, label=false)\n",
    "end\n",
    "scatter!(neur[1,1:m],    neur[2,1:m],    neur[3,1:m]    , markersize=4, markercolor=:red, label=false)\n",
    "scatter!(neur[1,m+1:2m], neur[2,m+1:2m], neur[3,m+1:2m] , markersize=4, markercolor=:blue, label=false)\n",
    "\n",
    "fn = \"$resultdir/neurons_lastiter_3D__azim$(azimuth)elev$(elevation).png\"\n",
    "savefig(pltt, fn)\n",
    "pltt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
