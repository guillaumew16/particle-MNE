{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Logging\n",
    "logger = ConsoleLogger(stdout)\n",
    "# debuglogger = ConsoleLogger(stderr, Logging.Debug)\n",
    "global_logger(logger)\n",
    "using JLD\n",
    "\n",
    "using Random\n",
    "rng = MersenneTwister(1234)\n",
    "import Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "pyplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "using ProgressMeter\n",
    "using LinearAlgebra: dot\n",
    "include(\"../utils/misc.jl\")\n",
    "using .MiscUtils: myCircle, mySphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = Dates.now()\n",
    "\n",
    "## model parameters\n",
    "input_dim = 2 # input dim\n",
    "bias = true\n",
    "m = 50 # nb hidden neurons for each sign\n",
    "N = 5 # nb samples\n",
    "activation_str = \"ReLUcub\"\n",
    "\n",
    "## logging\n",
    "ts_fsfriendly = Dates.format(ts, \"yyyy-mm-ddTHHMMSS\") # filesystem-friendly string for ts\n",
    "resultdir = mkpath(\"../results/maxF1margin__dim$(input_dim)bias$(bias)__m$(m)__N$(N)__$(activation_str)__$(ts_fsfriendly)\")\n",
    "\n",
    "logfile = \"$resultdir/log.txt\"\n",
    "touch(logfile)\n",
    "open(logfile, \"a\") do f\n",
    "    write(f, \"input_dim=$input_dim\\n\")\n",
    "    write(f, \"bias: $bias\\n\")\n",
    "    write(f, \"m=$m\\n\")\n",
    "    write(f, \"N=$N\\n\")\n",
    "    write(f, \"activation: $activation_str\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 8\n",
    "scaling = 1 # unkown what the \"correct\" scaling typically is!\n",
    "\n",
    "## algo parameters\n",
    "T = Int(floor(4000 / (alpha^scaling)))\n",
    "eta0_a = 1e-1 * alpha # initial stepsize\n",
    "eta0_w = 1e-1 * alpha\n",
    "eta0_theta = 1e-2 * alpha\n",
    "constrain_theta = true\n",
    "extrasteps = 2 # extrasteps=1: CP-MDA, extrasteps=2: CP-MP\n",
    "\n",
    "open(logfile, \"a\") do f\n",
    "    write(f, \"T=$T\\n\")\n",
    "    write(f, \"eta0_a=$(eta0_a)\\n\")\n",
    "    write(f, \"eta0_w=$(eta0_w)\\n\")\n",
    "    write(f, \"eta0_theta=$(eta0_theta)\\n\")\n",
    "    write(f, \"constrain_theta: $(constrain_theta)\\n\")\n",
    "    write(f, \"extrasteps=$(extrasteps)\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting parameters\n",
    "TNI_min = 1 # plot NI error\n",
    "TNI_max = T\n",
    "evalNIevery = Int(floor((TNI_max-TNI_min)/50))\n",
    "Ntheta = Int(1e4)\n",
    "skip_avg = true # avg not implemented\n",
    "\n",
    "@assert input_dim==2\n",
    "# r = range(-3, 3, length=101)\n",
    "r = range(-2.2, 2.2, length=101)\n",
    "\n",
    "Tplotreg_min = 1 # plot decision regions gif\n",
    "Tplotreg_max = T\n",
    "plotregevery = Int(floor((Tplotreg_max-Tplotreg_min)/50))\n",
    "hidetitle = true\n",
    "hidelabel = true\n",
    "skip_gifs = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if activation_str == \"ReLU\"\n",
    "    activation(x) = max(0, x)\n",
    "elseif activation_str == \"abs\"\n",
    "    activation(x) = abs(x)\n",
    "elseif activation_str == \"ReLUsq\"\n",
    "    activation(x) = max(0, x)^2\n",
    "elseif activation_str == \"ReLUcub\"\n",
    "    activation(x) = max(0, x)^3\n",
    "elseif activation_str == \"ReLUquar\"\n",
    "    activation(x) = max(0, x)^4\n",
    "# elseif activation_str == \"sq\" # polynomial activation is silly\n",
    "#     activation(x) = x^2\n",
    "elseif activation_str == \"sigmoid\"\n",
    "    activation(x) = 1/(1+exp(-x))\n",
    "elseif activation_str == \"tanh\"\n",
    "    activation(x) = tanh(x)\n",
    "else\n",
    "    error(\"activation_str not recognized, should be \n",
    "        \\\"ReLU\\\" or \n",
    "        \\\"abs\\\" or \n",
    "        \\\"ReLUsq\\\" or \n",
    "        \\\"ReLUcub\\\" or \n",
    "        \\\"ReLUquar\\\" or \n",
    "        \\\"sigmoid\\\" \n",
    "        or \\\"tanh\\\"\")\n",
    "end\n",
    "\n",
    "if bias == true\n",
    "    d = input_dim + 1\n",
    "else\n",
    "    d = input_dim\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function makedataset(d, N; bias, rng=MersenneTwister(1234))\n",
    "    x = randn(rng, d, N)\n",
    "    if bias\n",
    "        x[d,:] .= 1\n",
    "    end\n",
    "    y = sign.(rand(rng, N).-0.5) # -1 or 1 uniformly\n",
    "    return x, y\n",
    "end\n",
    "\n",
    "Random.seed!(rng, 1234)\n",
    "x, y = makedataset(d, N; bias=bias, rng=rng)\n",
    "save(\"$resultdir/dataset.jld\", \"x\", x, \"y\", y)\n",
    "\n",
    "scatter(x[1, y.==1], x[2, y.==1], m=:circ, markersize=8, label=\"+\", color=:green)\n",
    "scatter!(x[1, y.==-1], x[2, y.==-1], m=:utriangle, markersize=8, label=\"-\", color=:red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##   max_nu min_i int_Theta yi*phi(theta, xi) dnu(theta) subj. to nu in M(Theta) and ||nu||=1\n",
    "## = min_a max_nu sum_i int_{Theta+ U Theta-} ai yi*phitilde(theta, xi) dnu(theta) subj. to nu in P(Theta+ U Theta-) and a in P([N]), \n",
    "## where phitilde(theta, x) = phi(theta, x) if theta in Theta+ and -phi(theta, x) if theta in Theta- (cf Appdx A of Chizat21)\n",
    "# the network is parametrized such that the first m neurons are positively weighted and the last m are negatively weighted\n",
    "gfun(i, theta) = y[i] * activation(dot(theta, x[:,i]))\n",
    "\n",
    "function logit(x, w, theta)\n",
    "    out = 0\n",
    "    for j=1:m\n",
    "        out += w[j] * activation(dot(theta[:,j], x))\n",
    "    end\n",
    "    for j=m+1:2m\n",
    "        out += -w[j] * activation(dot(theta[:,j], x))\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "if bias\n",
    "    predict(x1, x2; w, theta, hard=true) = hard ? sign(logit([x1, x2, 1], w, theta)) : logit([x1, x2, 1], w, theta)\n",
    "else\n",
    "    predict(x1, x2; w, theta, hard=true) = hard ? sign(logit([x1, x2], w, theta)) : logit([x1, x2], w, theta)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(rng, 1234)\n",
    "# initialize adversary weights\n",
    "a = ones(N) ./ N\n",
    "# initialize network weights and positions nu=(w, theta); theta constrained to unit l2 sphere\n",
    "w = ones(2m) ./ (2m)\n",
    "theta = randn(rng, d, 2m)\n",
    "for j=1:2m\n",
    "    theta[:,j] ./= sqrt(sum(theta[:,j].^2))\n",
    "end\n",
    "\n",
    "# for extragradient step\n",
    "ap = similar(a)\n",
    "wp = similar(w)\n",
    "thetap = similar(theta)\n",
    "# to store intermediate values\n",
    "copies_a = Array{Float64}(undef, N, T+1)\n",
    "copies_w = Array{Float64}(undef, 2m, T+1)\n",
    "copies_theta = Array{Float64}(undef, d, 2m, T+1)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the decision region at random initialization https://discourse.julialang.org/t/plotting-decision-boundary-regions-for-classifier/21397\n",
    "contour(r, r, \n",
    "    (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "    f=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take a CP gradient step\n",
    "- starting from a, w, theta\n",
    "- evaluating the gradients at ap, wp, thetap\n",
    "- with stepsizes eta_a, eta_w, eta_theta\n",
    "(taking care of the fact that the network is parametrized such that the first m neurons are positively weighted and the last m are negatively weighted)\n",
    "Returns the updated particles a1, w1, theta1\n",
    "\"\"\"\n",
    "function step_CPMDA(\n",
    "        f,\n",
    "        a, w, theta,\n",
    "        ap, wp, thetap,\n",
    "        eta_a, eta_w, eta_theta;\n",
    "        true_prox=false,\n",
    "        constrain_theta=true\n",
    ")\n",
    "    N = length(a)\n",
    "    d = size(theta)[1]\n",
    "    m = Int(size(theta)[2] / 2)\n",
    "    \n",
    "    neuronsigns = ones(2m)\n",
    "    neuronsigns[m+1:2m] .= -1\n",
    "\n",
    "    Dfp = Array{Float64}(undef, d, N, 2m) # gradient of f w.r.t theta at thetap\n",
    "    for i=1:N, j=1:2m\n",
    "        Dfp[:,i,j] = ForwardDiff.gradient(tt -> f(i,tt), thetap[:,j])\n",
    "    end\n",
    "\n",
    "    # take step: adversary\n",
    "    s = Array{Float64}(undef, N)\n",
    "    for i=1:N\n",
    "        s[i] = sum( neuronsigns[j] * wp[j] * f(i, thetap[:,j]) for j=1:2m )\n",
    "    end\n",
    "    if eta_a == Inf\n",
    "        a1 = zeros(N)\n",
    "        a1[argmax(s)] = 1.\n",
    "    else\n",
    "        a1 = a .* exp.(-eta_a * s)\n",
    "        a1 ./= sum(a1)\n",
    "    end\n",
    "    \n",
    "    # take step: network\n",
    "    w1 = similar(w)\n",
    "    theta1 = similar(theta)\n",
    "    for j=1:2m\n",
    "        s = sum( ap[i] * f(i, thetap[:,j]) for i=1:N )\n",
    "        w1[j] = w[j] * exp(eta_w*neuronsigns[j]*s)\n",
    "    end\n",
    "    w1 ./= sum(w1)\n",
    "    for j=1:2m\n",
    "        s = sum( ap[i] * Dfp[:,i,j] for i=1:N )\n",
    "        @assert size(s) == (d,)\n",
    "        if true_prox\n",
    "            theta1[:,j] = theta[:,j] + eta_theta * neuronsigns[j] * wp[j] / w[j] * s\n",
    "        else\n",
    "            theta1[:,j] = theta[:,j] + eta_theta * neuronsigns[j] * s\n",
    "        end\n",
    "    end\n",
    "    # retract (just project) theta1 back to unit l2 sphere\n",
    "    if constrain_theta\n",
    "        for j=1:2m\n",
    "            theta1[:,j] ./= sqrt(sum(theta1[:,j].^2))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return a1, w1, theta1\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_a, eta_w, eta_theta = eta0_a, eta0_w, eta0_theta # constant stepsizes\n",
    "\n",
    "@showprogress 1 for t=1:T # minimum update interval of 1 second\n",
    "    copies_a[:,t] = copy(a)\n",
    "    copies_w[:,t] = copy(w)\n",
    "    copies_theta[:,:,t] = copy(theta)\n",
    "\n",
    "    # extragradient (\"ghost\" step)\n",
    "    # extrasteps=1: CP-MDA, extrasteps=2: CP-MP\n",
    "    ap, wp, thetap = a, w, theta\n",
    "    for s=1:extrasteps\n",
    "        ap, wp, thetap = step_CPMDA(    \n",
    "            gfun,\n",
    "            a, w, theta,\n",
    "            ap, wp, thetap,\n",
    "            eta_a, eta_w, eta_theta;\n",
    "            true_prox=true,\n",
    "            constrain_theta=constrain_theta\n",
    "        )\n",
    "    end\n",
    "\n",
    "    # take step\n",
    "    a, w, theta = ap, wp, thetap\n",
    "end\n",
    "\n",
    "copies_a[:,T+1] = copy(a)\n",
    "copies_w[:,T+1] = copy(w)\n",
    "copies_theta[:,:,T+1] = copy(theta)\n",
    "\n",
    "af, wf, thetaf = a, w, theta\n",
    "save(\"$resultdir/iterates.jld\", \"copies_a\", copies_a, \"copies_w\", copies_w, \"copies_theta\", copies_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the NI error\n",
    "\"\"\"\n",
    "Compute the \"global\" Nikaido-Isoda (NI) error\n",
    "    max_{a0, nu0} <a|F|nu0> - <a0|F|nu> = max_theta <a|F|delta_theta> - min_i <ei|F|nu>\n",
    "\"\"\"\n",
    "function glob_NI_err(gfun, a, w, theta; Ntheta=Int(1e4))\n",
    "    N = length(a)\n",
    "    d = size(theta)[1]\n",
    "    m = Int(size(theta)[2] / 2)\n",
    "    \n",
    "    if d==2\n",
    "        apcont = myCircle(Ntheta)\n",
    "    elseif d==3\n",
    "        apcont, Ntheta_new = mySphere(Ntheta)\n",
    "    else\n",
    "        error(\"glob_NI_err not implemented for d>3\")\n",
    "    end\n",
    "    maxtheta = -Inf\n",
    "    mintheta = +Inf\n",
    "    for k=1:Ntheta_new\n",
    "        theta0 = apcont[:,k]\n",
    "        s = 0\n",
    "        for i=1:N\n",
    "            s += a[i] * gfun(i, theta0)\n",
    "        end\n",
    "        maxtheta = max(maxtheta, s)\n",
    "        mintheta = min(mintheta, s)\n",
    "    end\n",
    "    Fnu(i) = sum( w[j] * gfun(i, theta[:,j]) for j=1:m ) - sum( w[j] * gfun(i, theta[:,j]) for j=m+1:2m )\n",
    "    mini = minimum(Fnu(i) for i=1:N)\n",
    "    return max(maxtheta, -mintheta) - mini\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TNI_max = 400\n",
    "evalNIevery = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nierrs = Array{Float64}(undef, T+2)\n",
    "@showprogress 1 for t=TNI_min:evalNIevery:TNI_max+1 # minimum update interval of 1 second\n",
    "# for t=TNI_min:evalNIevery:TNI_max+1\n",
    "    nierrs[t] = glob_NI_err(gfun, copies_a[:,t], copies_w[:,t], copies_theta[:,:,t]; Ntheta=Ntheta)\n",
    "end\n",
    "if !skip_avg\n",
    "    nierrs[T+2] = glob_NI_err(gfun, avg_a, avg_w, avg_theta; Ntheta=Ntheta)\n",
    "end\n",
    "open(logfile, \"a\") do f\n",
    "    for t=TNI_min:evalNIevery:TNI_max+1\n",
    "        write(f, \"glob_NI_err at iteration#$t: $(nierrs[t])\\n\")\n",
    "    end\n",
    "    if !skip_avg \n",
    "        write(f, \"glob_NI_err at avg iterate: $(nierrs[T+2])\\n\") \n",
    "    end\n",
    "end\n",
    "\n",
    "save(\"$resultdir/nierrs__every$(evalNIevery)__t=$(TNI_min)--$(TNI_max).jld\", \"nierrs\", nierrs)\n",
    "\n",
    "plt_NI = plot(range(TNI_min, stop=TNI_max+1, step=evalNIevery), nierrs[TNI_min:evalNIevery:TNI_max+1], xlabel=\"k\", label=\"\")\n",
    "if !skip_avg\n",
    "    hline!([nierrs[T+2]], label=\"avg iterate\")\n",
    "end\n",
    "if !hidetitle\n",
    "    title!(\"NI error of iterates\")\n",
    "end\n",
    "fn = \"$resultdir/NI_errors.png\"\n",
    "savefig(plt_NI, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10 # numerical stability (we use approximations (with deltax, deltay) to compute glob_NI_err)\n",
    "upscale=2   # adjusting fontsize, linewidths etc.\n",
    "resscale=2  # keep everything else fixed but increase resolution\n",
    "fontsize=11/upscale*1.75\n",
    "\n",
    "plt_NI_log = plot(range(TNI_min, stop=TNI_max+1, step=evalNIevery), eps .+ max.(0, nierrs[TNI_min:evalNIevery:(TNI_max+1)]), xlabel=\"k\", label=\"\", yscale=:log10,\n",
    "    linewidth=upscale,\n",
    "    xtickfontsize=fontsize, ytickfontsize=fontsize, xguidefontsize=fontsize, yguidefontsize=fontsize, legendfontsize=fontsize,\n",
    "    dpi=upscale*100*resscale,\n",
    "    size=(600/upscale, 400/upscale))\n",
    "if !skip_avg\n",
    "    hline!([ eps + max(0, nierrs[T+2]) ], label=(hidelabels ? \"\" : \"avg iterate\"))\n",
    "end\n",
    "if !hidetitle\n",
    "    title!(\"NI error of iterates (log-linear scale)\")\n",
    "end\n",
    "fn = \"$resultdir/NI_errors_logscale.png\"\n",
    "savefig(plt_NI_log, fn)\n",
    "\n",
    "plt_NI_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decision region at the last iterate\n",
    "\n",
    "upscale=2   # adjusting fontsize, linewidths etc.\n",
    "resscale=2  # keep everything else fixed but increase resolution\n",
    "fontsize=11/upscale*1.75\n",
    "\n",
    "pltt = contour(r, r,\n",
    "    (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "    f=true,\n",
    "    levels=-6:1.:6,\n",
    "    clims=(-6, 6),\n",
    "    linewidth=upscale,\n",
    "    xtickfontsize=fontsize, ytickfontsize=fontsize, xguidefontsize=fontsize, yguidefontsize=fontsize, legendfontsize=fontsize*0.8,\n",
    "    dpi=upscale*100*resscale,\n",
    "    size=(600/upscale, 500/upscale))\n",
    "contour!(r, r,\n",
    "    (x1, x2) -> predict(x1, x2; w=w, theta=theta, hard=false),\n",
    "    levels=[0.],\n",
    "    seriescolor=:blues,\n",
    "    linestyle=:dash,\n",
    "    linewidth=3/upscale)\n",
    "scatter!(x[1, y.==1], x[2, y.==1], m=:circ, markersize=8/upscale, markerstrokewidth=1/upscale, label=\"+\", color=:green)  \n",
    "scatter!(x[1, y.==-1], x[2, y.==-1], m=:utriangle, markersize=8/upscale, label=\" -\", markerstrokewidth=1/upscale, color=:red)\n",
    "\n",
    "fn = \"$resultdir/contour_soft_lastiter.png\"\n",
    "savefig(pltt, fn)\n",
    "pltt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots.PlotMeasures # enable setting \"margins=10mm\" in plots\n",
    "using LaTeXStrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscale=2   # adjusting fontsize, linewidths etc.\n",
    "resscale=2  # keep everything else fixed but increase resolution\n",
    "fontsize=11/upscale*1.4\n",
    "\n",
    "azimuth = 25\n",
    "elevation = 55\n",
    "\n",
    "phi_s = range(0, 2pi, length=100)\n",
    "theta_s = range(0, pi, length=100)\n",
    "r0 = 1/m\n",
    "xss = r0 .* [cos(t)*sin(p) for t in theta_s, p in phi_s]\n",
    "yss = r0 .* [sin(t)*sin(p) for t in theta_s, p in phi_s]\n",
    "zss = r0 .* [cos(p) for t in theta_s, p in phi_s]\n",
    "pltt = plot(xss, yss, zss, linetype=:surface, colorbar=false, seriescolor=:greens, alpha=0.1, \n",
    "    xlabel=L\"θ_1\", ylabel=L\"θ_2\", zlabel=L\"θ_3\",\n",
    "    xlim=(-0.05, 0.05),\n",
    "    ylim=(-0.05, 0.05),\n",
    "    zlim=(-0.05, 0.05),\n",
    "    camera = (azimuth, elevation),\n",
    "    margins=10mm,\n",
    "    xtickfontsize=0.6*fontsize, ytickfontsize=0.6*fontsize, ztickfontsize=0.6*fontsize,\n",
    "    xguidefontsize=fontsize, yguidefontsize=fontsize, zguidefontsize=fontsize,\n",
    "    legendfontsize=fontsize*0.8,\n",
    "    dpi=upscale*100*resscale,\n",
    "    size=(600/upscale, 600/upscale),\n",
    "    aspect_ratio=:equal)\n",
    "theta = thetaf\n",
    "neur = zeros(3, 2m)\n",
    "for j=1:2m\n",
    "    neur[:,j] = w[j] * theta[:,j]\n",
    "end\n",
    "for j=1:m\n",
    "    plot!([0, neur[1,j]], [0, neur[2,j]], [0, neur[3,j]], color=:red, label=false)\n",
    "end\n",
    "for j=m+1:2m\n",
    "    plot!([0, neur[1,j]], [0, neur[2,j]], [0, neur[3,j]], color=:blue, label=false)\n",
    "end\n",
    "scatter!(neur[1,1:m],    neur[2,1:m],    neur[3,1:m]    , markersize=4, markercolor=:red, label=false)\n",
    "scatter!(neur[1,m+1:2m], neur[2,m+1:2m], neur[3,m+1:2m] , markersize=4, markercolor=:blue, label=false)\n",
    "\n",
    "fn = \"$resultdir/neurons_lastiter_3D__azim$(azimuth)elev$(elevation).png\"\n",
    "savefig(pltt, fn)\n",
    "pltt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
